---
title: "Intro"
author: "SA23204186"
date: "2023/12/4"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SA23204182}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Rpackage

the r function generate SBM model in r package

```{r,eval=FALSE}
generater_model<-function(B=matrix(c(.2,.3,.3,.1),nrow = 2),p=c(0.3,0.7),n=1000){
  if(nrow(B)!=ncol(B))
    return("ERROR")
  m<-nrow(B)
  for (i in 1:m) {
    for (j in 1:m) {
      if(B[i,j]!=B[j,i])
        return("ERROR")
      if(B[i,j]<0)
        return("ERROR")
    }
    
  }
  g<- sample(1:m, size = n, replace = TRUE, prob = p)
  A<-matrix(0,ncol = n,nrow = n)
  for (i in 1:(n-1)) {
    for (j in (i+1):n) {
      A[i,j]<-rbinom(1,1,B[g[i],g[j]])
      A[j,i]<-A[i,j]
    }
  }
  return(A)
}



```

Adjacency spectral embedding and Laplacian embedding

```{r,eval=FALSE}
ASE <- function(A,d,lapace="FALSE") {
if(nrow(A)!=ncol(A))
  return("ERROR")
n=nrow(A)
for (i in 1:n) {
  for (j in 1:n) {
    if(A[i,j]!=A[j,i])
      return("ERROR")
    if(A[i,j]<0)
      return("ERROR")
  }
  
}
for (i in 1:n) {
  A[i,i]<-0
}

if(lapace=="TRUE"){
  D<-matrix(0,nrow = n,ncol = n)
  for (i in 1:n) {
    D[i,i]=1/sqrt(sum(A[,i]))
  }
  A<-D%*%A%*%D
}

E<-eigen(A)
s<-E$values
v<-E$vectors
v1<-matrix(0,nrow = n,ncol = d)
for (i in 1:d) {
  v1[,i]<-v[,i]*sqrt(abs(s[i]))
}
return(v1)
}



```

# HW1

## Problem1

\n

Write the "my.sample" function to act as the function "sample" with
replace true. \n

```{r}
set.seed(12345)
my.sample<-function(n,A,p){## n numbers of times you wan to test,A the value that the random virable can take, p the probablity coressponding to A it should be as long as A,and sumed to 1.
  if(sum(p)!=1||(length(A)!=length(p)))
    return("ERROR")
  else
    b<-runif(n)
    c<-rep(A[1],n)
    d=0
    for (i in 1:n) {d=0
      for (j in 1:length(p)) {
        d=d+p[j]
        if(d>b[i]){
          c[i]<-A[j]
          break
        }
      }
    }
  return(c)
}
##some examples of this function
my.sample(50,c(1,2,3),c(0.1,0.2,0.7))
my.sample(40,letters[1:4],c(0.1,0.2,0.3,0.4))

```

## 3.2

\n

Use the inverse transform method to generate a random sample of size
1000 from standard Laplace distribution. \n

```{r }
n <- 1000
set.seed(12345)
u <- runif(n)
x <- sign(1/2-u)*log(1-abs(1-2*u)) # F(x) = 1/2*exp(-x), 0<=x<=1/2,F(x)=1-1/2*exp(-x),1/2<x<=1
hist(x, prob = TRUE)
z<-seq(-10,0,.01)
fz<-1/2*exp(z)
lines(-z,fz)
lines(z,fz)
```

## 3.7

\n

Write a function to generate a random sample of size n from the Beta(a,
b) distribution by the acceptance-rejection. method.Generate a random
sample of size 1000 from the Beta(3,2) distribution. Graph the histogram
of the sample with the theoretical Beta(3,2) density superimposed. \n

```{r}
set.seed(1235)
my.beta<-function(n,a,b){## Let g(x=1),c=beta(a,b)^(-1)
j<-k<-0;y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1) #random variate from g(.)
if (x^(a-1) * (1-x)^(b-1) > u) {
#we accept x
k <- k + 1
y[k] <- x
}
}
  return(y)

}
y<-my.beta(1000,3,2)
hist(y, prob = TRUE)
z<-seq(0,1,0.01)
fz<-beta(3,2)^(-1)*(z^2)*(1-z)
lines(z,fz)
```

\n

## 3.9

\n

Write a function to generate random variates from fe. (fe=3/4(1-x\^2))
\n

```{r}
set.seed(12453)
my.fe<-function(n){
  x<-runif(3*n,min = -1,max = 1)
  y<-numeric(n)
  for (i in 1:n) {
    if((abs(x[3*i])>abs(x[3*i-1]))&&(abs(x[3*i])>abs(x[3*i-2])))
      y[i]<-x[3*i-1]
    else
      y[i]<-x[3*i]
  }
  return(y)
}
y<-my.fe(10000)
hist(y, prob = TRUE)
z<-seq(-1,1,0.01)
fz<-3/4*(1-z^2)
lines(z,fz)
```

## 3.10

\n

Prove that the algorithm given in Exercise 3.9 generates variates from
the density fe. \n

### proof

\n

We suppose X is the random virable that generated by methods,we can see
that X is symmetric.So to prove this conclusion, we just need to prove
that \|X\| has the density f=3/2(1-x\^2),which is equal to prove \|X\|
has the distribution$$ F_{|X|}(x)=\frac{3}{2}(x-\frac{1}{3}x^{3}).$$\n
We consider the probability
$$P(|X|\leq x)=\frac{1}{2}P(|U|_{(1)}\leq x)+\frac{1}{2}P(|U|_{(2)}\leq x)\\
     =\frac{1}{2}(1-(1-x)^{3})+\frac{1}{2}\times 6\int_{0}^{x}y(1-y)dy\\
     =\frac{1}{2}(1-(1-x)^{3})+\frac{1}{2}(3x^{2}-2x^{3})\\
     =\frac{3}{2}(x-\frac{1}{3}x^{3}).$$ \n As X is symmetirc, so we
prove the conclusion.

# HW2

## 1

\n

Proof that what value $\rho=\frac{l}{d}$ should be take to minimize the
asymptotic variance of
$\hat{\pi}=\frac{2ln}{dm},m\sim B(n,\frac{2l}{d\pi})$ . \n

### answer

\n

We have the
equation:$$ \hat{\pi}=\frac{2ln}{dm},\mathrm{E}[m]=n\frac{2l}{d\pi},Var(m)=n\frac{2l}{d\pi}(1-\frac{2l}{d\pi})$$so
,by using delta method ,we have
$$Var(\hat{\pi})\approx (\frac{2ln}{d(\mathrm{E}[m])^{2}})^{2} Var(m)=\frac{\pi ^{2}(\pi d-2l)}{2ln}$$.\n
So we should choose l/d=1.\n

We than use MC method to compare the variance under different l/d. \n

```{r}
K=100
p<-matrix(0,nrow = 3,ncol = K)
l <- c(0.5,0.8,1)
d <- 1
n <- 1e6
for (i in 1:K) {
     X <- runif(n,0,d/2)
     Y <- runif(n,0,pi/2)
  for (j in 1:3) {
    p[j,i]<-2*l[j]/d/mean(l[j]/2*sin(Y)>X)
  }
}
cat("l/d:",l,"\n","var:",c(var(p[1,]),var(p[2,]),var(p[3,])))

```

## 5.6

\n

In Example 5.7 the control variate approach was illustrated for Monte
Carlo integration .Now consider the antithetic variate approach.Compute
$Cov(e^{U},e^{1-U})$ and $Var(e^{U}+e^{1-U})$,where
$U\sim Uniform(0,1)$.What is the percent reduction in variance of
$\hat{\theta}$ that can be achieved using antithetic variates (compared
with simple MC)?\n

### answer

\n

We have
$$Cov(e^{U},e^{1-U})=\mathrm{E}[e^{U}e^{1-U}]-\mathrm{E}e^{U}\mathrm{E}e^{1-U}\\
=e-(e-1)^{2},$$ and
$$Var(e^{U}+e^{1-U}) =Var(e^{U})+Var(e^{1-U})+2Cov(e^{U},e^{1-U})\\
=e^{2}-1-2(e-1)^{2}+2e-2(e-1)^{2}\\
=4e-2-3(e-1)^{2}\\\approx 0.1565.$$ \n

So we can get the variance of $\hat{\theta}$ is
$$Var(\hat{\theta})=\frac{Var(\frac{e^{U}+e^{1-U}}{2})}{m} \\
=\frac{4e-2-3(e-1)^{2}}{4m}$$,while the simple MC method has the
variance $$Var(\hat{\theta}_{Simple MC})=\frac{Var(e^{U})}{m} \\
=\frac{2e-2-(e-1)^{2}}{2m}$$.\n

So the percent reduction is
$$1-\frac{Var(\hat{\theta})}{Var(\hat{\theta}_{SimpleMC})} \\
=1-\frac{4e-2-3(e-1)^{2}}{4e-4-2(e-1)^{2}}\\
=\frac{(e-1)^{2}-2}{4e-4-2(e-1)^{2}}\\\approx 0.984$$

## 5.7

\n

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method.
Compute an empirical estimate of the percent reduction in variance using
the antithetic variate. Compare the result with the theoretical value
from Exercise 5.6.

\n

### answer

\n

```{r}
K=100
n=1e5
p<-matrix(0,nrow = 2,ncol = K)
for (i in 1:K) {
  U<-runif(n)
  p[1,i]=var((exp(U)+exp(1-U))/2)
  p[2,i]=var(exp(U))
}
cat("simple MC approach","  antithetic variate approach","\n","var:",c(mean(p[2,])/n,mean(p[1,])/n),"\n")
cat("percent reduction:",1-mean(p[1,])/mean(p[2,]))

```

## HW3

## 1

Prove that if g is a continous function over $(a_{i},b_{i})$,then
$\frac{Var(\hat{\theta}_{S})}{Var(\hat{\theta}_{M})}\to 0$,as
$b_{i}-a_{i}\to 0$. \n

### proof

\n

As g is continous ,we can see that,when$\lim_{y\to 0}g(x+y)=g(x)$.So
that,$$\lim_{b_{i}-a_{i}\to 0}Var(g(U)|I=i)=\lim_{b_{i}-a_{i}\to 0} \int_{a_{i}}^{b_{i}}(g(u)-\theta_{i})^{2}du\\
=\lim_{b_{i}-a_{i}\to 0}\int_{a_{i}}^{b_{i}}g(u)^{2}du-(\int_{a_{i}}^{b_{i}}g(u)du)^{2}\\
=g(a_{i})^{2}-g(a_{i})^{2}\\
=0$$

\n

From then on ,we notice $\theta_{M}$ has nothing to do with every
$a_{i},b_{i}$,so we get the proof.

## 5.13

Find two importance functions f1 and f2 that are supported on (1, ∞) and
are 'close' to g(x),Which of your two importance functions should
produce the smaller variance in estimating the integration by importance
sampling?

\n

### answer

\n

We choose $f_{1}(x)=e^{1-x}$ and
$f_{2}(x)=\frac{2e^{-\frac{(1-x)^{2}}{2}}}{\sqrt{2\pi}}$. we find that
$$\lim_{x\to\infty }\frac{\frac{g(x)}{f_{2}(x)}}{\frac{g(x)}{f_{1}(x)}}=\frac{x^{2}}{e^{x-1+0.5x^{2}}}=0$$
\n So, we can see that f2 should produce the smaller variance in
estimating the integration by importance sampling.

\n

## 5.14

\n

Obtain a Monte Carlo estimate of $\int_{1}^{\infty}g(x)dx$ by importance
sampling,where g is defined above.

\n

We choose the importance function f2 defined above.

```{r}
n=1e5
A<-matrix(0,nrow = 2,ncol = n)
a<-rnorm(n)
a<-1+abs(a)
theta<-a^{2}*exp(-0.5*a^{2})/(2*exp(-0.5*(1-a)^{2}))
print("the integration is ")
print(mean(theta))

```

## 5.15

\n

Obtain the stratified importance sampling estimate in Example 5.13 and
compare it with the result of Example 5.10 \n

```{r}
n=1e5
a=0
b=rep(0,5)
k=1
A<-matrix(0,nrow = 5,ncol = 2000)
for (j in 1:5) {
  b[j]=integrate(exp,lower = -1*j/5,upper = -1*(j-1)/5)$value
  k=1
  while (k<2001) {
    a=rexp(1)
    if((a<j/5)&&(a>(j-1)/5)){
      A[j,k]<-a
      k<-k+1
    }
  }
}

Mean<-rep(0,5)
for (j in 1:5) {
  Mean[j]<-mean(b[j]/(1+A[j,]^2))
}
Var<-rep(0,5)
for (j in 1:5) {
  Var[j]<-var(b[j]/(1+A[j,]^2))
}
print("the integration is ")
print(sum(Mean))
print("the variance of the estimate is ")
print(sum(Var)/2000)

```

## 6.5

Use a Monte Carlo experiment to estimate the coverage probability of the
t-interval for random samples of χ2(2) data with sample size n = 20.
Compare your t-interval results with the simulation results in Example
6.4.

```{r}
n <- 20
alpha <- .05
U <- replicate(1000, expr = {
x <- rchisq(n, df = 2)
c(mean(x)+sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE),mean(x)-sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE))
} )
k=0
for (i in 1:1000) {
  if(U[1,i]>2&&U[2,i]<2){
    k<-k+1
  }
}
print(k/1000)

```

We can see that it is more close to the situation of normal population
than that in example 6.5.

## 6.A

Use Monte Carlo simulation to investigate whether the empirical Type I
error rate of the t-test is approximately equal to the nominal
significance level α, when the sampled population is non-normal. The
t-test is robust to mild departures from normality. Discuss the
simulation results for the cases where the sampled population is (i)
χ2(1), (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case,
test H0 : µ = µ0 vs H0 : µ = µ0, where µ0 is the mean of χ2(1),
Uniform(0,2), and Exponential(1), respectively.

```{r}
cat("In this simulation,we choose alpha=0.05","\n")
n <- 20
alpha <- .05
U <- replicate(1000, expr = {
x <- rchisq(n, df = 1)
c(mean(x)+sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE),mean(x)-sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE))
} )
k=0
for (i in 1:1000) {
  if(U[1,i]>1&&U[2,i]<1){
    k<-k+1
  }
}
cat("The Type I error rate with χ2(1) being the sample population:",1-k/1000,"\n")


n <- 20
alpha <- .05
U <- replicate(1000, expr = {
x <- runif(n,min = 0,max = 2)
c(mean(x)+sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE),mean(x)-sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE))
} )
k=0
for (i in 1:1000) {
  if(U[1,i]>1&&U[2,i]<1){
    k<-k+1
  }
}
cat("The Type I error rate with Uniform(0,2) being the sample population:",1-k/1000,"\n")

n <- 20
alpha <- .05
U <- replicate(1000, expr = {
x <- rexp(n)
c(mean(x)+sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE),mean(x)-sqrt(var(x)/n)*qt(alpha/2,df=n-1,lower.tail = FALSE))
} )
k=0
for (i in 1:1000) {
  if(U[1,i]>1&&U[2,i]<1){
    k<-k+1
  }
}
cat("The Type I error rate with Exponential(1) being the sample population:",1-k/1000,"\n")
```

So,we can see that the t-test is robust to mild departures from
normality.

# HW4

## 1

\n

Compare B-F and B-H in m=1000 tests by computing FWER,FDR and TPR with
M=1000 simulations.

```{r}
set.seed(12345)
M=1000
m=1000
BF<-matrix(0,ncol = 3,nrow = M)
BH<-matrix(0,ncol = 3,nrow = M)
a<-numeric(m*(1-0.05))
b<-numeric(m*0.05)
c<-numeric(m)
for (i in 1:M) {
  a<-runif(m*(1-0.05))
  b<-rbeta(m*0.05,shape1=0.1,shape2=1)
  c<-c(a,b)
  bf<-p.adjust(c,method = "bonferroni")
  bh<-p.adjust(c,method = "BH")
  BF[i,1]<-sum(bf[1:950]<0.1)>0
  BF[i,2]<-sum(bf[1:950]<0.1)/sum(bf<0.1)
  BF[i,3]<-sum(bf[1:950]>0.1)/950
  BH[i,1]<-sum(bh[1:950]<0.1)>0
  BH[i,2]<-sum(bh[1:950]<0.1)/sum(bf<0.1)
  BH[i,3]<-sum(bh[1:950]>0.1)/950
}
```

```{r,results='hold'}
cat("TYPE\t","FWER\t","FDR\t","TPR","\n")
cat("BF \t",round(mean(BF[,1]),3),"\t",round(mean(BF[,2]),3),"\t",round(mean(BF[,3]),3),"\n")
cat("BH \t",round(mean(BH[,1]),3),"\t",round(mean(BH[,2]),3),"\t",round(mean(BH[,3]),3),"\n")
```

## 2

\n

The theoretical bias is $\frac{\lambda}{n-1}$,the theoretical standard
error is $\frac{\lambda n}{(n-1)\sqrt(n-2)}$

```{r}
n<-c(5,10,20)
B=1000
m=1000
 MEAN<-matrix(0,ncol =3 ,nrow =m )
 VAR<-matrix(0,ncol = 3,nrow = m)
for (i in 1:m) {
 
  for (j in 1:3) {
    x<-rexp(n[j],rate = 2)
    lam<-numeric(B)
    for (k in 1:B) {
      y<-sample(x,replace = TRUE)
      lam[k]<-1/mean(y)
    }
    MEAN[i,j]<-mean(lam)-1/mean(x)
    VAR[i,j]<-sqrt(var(lam))
  }
}
me<-c(mean(MEAN[,1]),mean(MEAN[,2]),mean(MEAN[,3]))
va<-c(mean(VAR[,1]),mean(VAR[,2]),mean(VAR[,3]))
```

The results are as follow.

```{r,results='hold'}
cat("n","\t\t\t","5","\t","10","\t","20","\n")
cat("theoretical bias","\t",round(2/(n[1]-1),3),"\t",round(2/(n[2]-1),3),"\t",round(2/(n[3]-1),3),"\n")
cat("bootstrap bias","\t\t",round(me[1],3),"\t",round(me[2],3),"\t",round(me[3],3),"\n")
cat("theoretical sd","\t\t",round(2*n[1]/((n[1]-1)*sqrt(n[1]-2)),3),"\t",round(2*n[2]/(n[2]-1)/sqrt(n[2]-2),3),"\t",round(2*n[3]/(n[3]-1)/sqrt(n[3]-2),3),"\n")
cat("bootstrap sd","\t\t",round(va[1],3),"\t",round(va[2],3),"\t",round(va[3],3),"\n")
```

We can see that the bootstrap bias is more close to the theoretical bias
as n increase and the same to sd.

## 7.3

Obtain a bootstrap t confidence interval estimate for the correlation
statistic in Example 7.2 (law data in bootstrap). \n

### answer

\n

We apply the function in example 7.12,we calculate the 0.95 bootstrap
t-interval for correlation.

```{r}
library(bootstrap)

dat1<-matrix(0,nrow = 15,ncol = 2)
for (i in 1:15) {
  dat1[i,1]<-law[i,1]
  dat1[i,2]<-law[i,2]
}
boot.t.ci <-
function(x, B = 2000, R = 200, level = .95, statistic){
#compute the bootstrap t CI
x <- as.matrix(x)
n <- nrow(x)
stat <- numeric(B)
se1 <- numeric(B)
boot.se <- function(x, R, f) {
#local function to compute the bootstrap
#estimate of standard error for statistic f(x)
x <- as.matrix(x); m <- nrow(x)
th <- replicate(R, expr = {
i <- sample(1:m, size = m, replace = TRUE)
f(x[i, ])
})
return(sd(th))
}
for (b in 1:B) {
j <- sample(1:n, size = n, replace = TRUE)
y <- x[j, ]
stat[b] <- statistic(y)
se1[b] <- boot.se(y, R = R, f = statistic)
}
stat00 <- statistic(x)
t.stats <- (stat - stat00) / se1
se00 <- sd(stat)
alpha <- 1 - level

Qt <- quantile(t.stats, probs=c(alpha/2, 1-alpha/2),na.rm = TRUE, type = 1)
names(Qt) <- rev(names(Qt))
CI <- rev(stat00 - Qt * se00)
}


stat <- function(p) {
cor(p[,1],p[,2]) }
ci <- boot.t.ci(dat1, statistic = stat, B=2000, R=200)
print(ci)



```

# HW5

## 7.5

\n

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for
the mean time between failures 1/λ by the standard normal, basic,
percentile, and BCa methods. Compare the intervals and explain why they
may differ. \n

```{r}
x<-c(3,5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
library("boot")
ci.norm<-ci.basic<-ci.perc<-ci.bca<-numeric(2)
boot.mean<-function(y,i) mean(y[i])
de <- boot(data=x,statistic=boot.mean, R = 1000)
ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
ci.norm<-ci$norm[2:3];ci.basic<-ci$basic[4:5]
ci.perc<-ci$percent[4:5];ci.bca<-ci$bca[4:5]

```

```{r,results='hold'}
cat("bootstrap interval\n")
cat("norm\t",ci.norm,"\n")
cat("basic\t",ci.basic,"\n")
cat("perc\t",ci.perc,"\n")
cat("bca\t",ci.bca,"\n")
```

Because the data is distributed in exp(λ) not in Gauss distribution,so
the inerval may be different.

## 7.8

\n

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and
standard error of ˆθ.

\n

```{r}
library("bootstrap")
S<-cov(scor)
en<-eigen(S)
theta<-en$values[1]/sum(en$values)


n <- 100
theta.hat1 <- numeric(n)
for (i in 1:n) {
  scor.jack1 <- scor[-i, ]
  sigma.hat1 <- cov(scor.jack1)
  s1 <- eigen(sigma.hat1)
  v1 <- s1$values
  theta.hat1[i] <- v1[1]/sum(v1)
}
bias.jack <- (n-1)*(mean(theta.hat1)-theta)
se.jack <- sqrt((n-1)*mean((theta.hat1-theta)^2))
print(round(c(original = theta, bias.jack = bias.jack, se.jack = se.jack), 4))
```

## 7.11

\n

In Example 7.18, leave-one-out (n-fold) cross validation was used to
select the best fitting model. Use leave-two-out cross validation to
compare the models.

\n

```{r}
set.seed(12346)
ironslag<-read.csv("ironslag.csv")
m<-ironslag$magnetic
c<-ironslag$chemical
n<-length(m)

e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
# for n-fold cross validation
# fit models on leave-one-out samples
for (i in 1:(n-1)) {
  for(j in (i+1):n){
y <- m[-c(i,j)]
x <- c[-c(i,j)]
J1 <- lm(y ~ x)
yhat11 <- J1$coef[1] + J1$coef[2] * c[i]
e1[i*(i-1)/2+j-i] <- (m[i] - yhat11)^2
yhat12 <- J1$coef[1] + J1$coef[2] * c[j]
e1[i*(i-1)/2+j-i] <-e1[i*(i-1)/2+j-i] +(m[j] - yhat12)^2

J2 <- lm(y ~ x + I(x^2))
yhat21 <- J2$coef[1] + J2$coef[2] * c[i] +
J2$coef[3] * c[i]^2
e2[i*(i-1)/2+j-i] <- (m[i] - yhat21)^2
yhat22 <- J2$coef[1] + J2$coef[2] * c[j] +
J2$coef[3] * c[j]^2
e2[i*(i-1)/2+j-i] <-e2[i*(i-1)/2+j-i] +(m[j] - yhat22)^2

J3 <- lm(log(y) ~ x)
logyhat31 <- J3$coef[1] + J3$coef[2] * c[i]
yhat31 <- exp(logyhat31)
e3[i*(i-1)/2+j-i] <- (m[i] - yhat31)^2
logyhat32 <- J3$coef[1] + J3$coef[2] * c[j]
yhat32 <- exp(logyhat32)
e3[i*(i-1)/2+j-i] <- e3[i*(i-1)/2+j-i] +(m[j] - yhat32)^2


J4 <- lm(log(y) ~ log(x))
logyhat41 <- J4$coef[1] + J4$coef[2] * log(c[i])
yhat41 <- exp(logyhat41)
e4[i*(i-1)/2+j-i] <- (m[i] - yhat41)^2
logyhat42 <- J4$coef[1] + J4$coef[2] * log(c[j])
yhat42 <- exp(logyhat42)
e4[i*(i-1)/2+j-i] <- e4[i*(i-1)/2+j-i] +(m[j] - yhat42)^2
  }
}

```

```{r,results='hold'}
cat("model1\t","model2\t","model3\t","model4\n")
cat(round(mean(e1),2),"\t",round(mean(e2),2),round(mean(e3),2),round(mean(e4),2))
```

So we choose the model2

# HW6

## 1

\n

Proof the Stationarity of Metropolis-Hastings sampler Algorithm in
continuous situation. \n \### answer

\n

For the contimous situation:we
have$$f(r)\alpha(r,s)g(s|r)=f(r)g(s|r)min\{\frac{f(s)g(r|s)}{f(r)g(s|r)},1\}\\
=f(s)g(r|s)min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}\\
=f(s)g(r|s)\alpha(s,r) $$.

\n

If s=r,we have
$$f(r)1(s=r)[1-\int_{s}\alpha(r,s)g(s|r)]=f(s)1(r=s)[1-\int_{r}\alpha(s,r)g(r,s)] $$.

\n

So, we have the conclusion:$$K(s,r)f(r)=K(r,s)f(s) $$

## 8.1

Implement the two-sample Cram´er-von Mises test for equal distributions
as a permutation test. Apply the test to the data in Examples 8.1 and
8.2.

```{r}
X<-c(158,171 ,193, 199, 230, 243, 248, 248, 250, 267, 271, 316, 327, 329)
Y<-c(141 ,148, 169, 181, 203, 213, 229, 244, 257, 260, 271, 309)
library("twosamples")
out1<-cvm_test(X,Y)
print(out1)
summary(out1)

```

## 8.3

The Count 5 test for equal variances in Section 6.4 is based on the
maximum number of extreme points. Example 6.15 shows that the Count 5
criterion is not applicable for unequal sample sizes. Implement a
permutation test for equal variance based on the maximum number of
extreme points that applies when sample sizes are not necessarily equal.

```{r}
n1 <- 100
n2 <- 150
set.seed(12456)
m <- 500
c5test <- function(x, y, s) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))

return(as.integer(max(c(outx, outy)) > s))
}
x <- rnorm(n1)
y <- rnorm(n2)
s <- 5:15
R <- 100
q <- numeric(R)
alphahat <- pwr <- numeric(length(s))
for (j in 1:length(s)) {
  ss1 <- s[j]
  alphahat[j] <- c5test(x, y, ss1) 
  z <- c(x, y)
  K <- 1:(n1+n2)
  n<-length(x)
  for (i in 1:R) {
  k <- sample(K, size = n, replace = FALSE)
  x1 <- z[k]; y1 <- z[-k] 
  x1 <- x1 - mean(x1) 
 
  y1 <- y1 - mean(y1)
  q[i] <- c5test(x1, y1, ss1)
 }
 pwr[j] <- mean(c(alphahat[j], q))
}
plot(s, pwr, col = "black")



```

# HW7

## 1

1.  Consider a model P(Y = 1 \| X1, X2, X3) = exp(a+b1X1+b2X2+b3X3)
    1+exp(a+b1X1+b2X2+b3X3) , where X1 ∼ P(1), X2 ∼ Exp(1) and X3 ∼ B(1,
    0.5). • Design a function that takes as input values N, b1, b2, b3
    and f0, and produces the output a. • Call this function, input
    values are N = 10\^6 , b1 = 0, b2 = 1, b3 = −1, f0 = 0.1, 0.01,
    0.001, 0.0001. • Plot − log f0 vs a.

```{r}
fa1<-function(N,b1,b2,b3,f0){
  an<-numeric(N)
  for (i in 1:N) {
    x1<-rpois(n=1,lambda = 1)
    x2<-rexp(n=1,rate = 1)
    x3<-rbinom(1,1,prob = 0.5)
    an[i]<-log(f0/(1-f0))-b1*x1-b2*x2-b3*x3
  }
  return(mean(an))
  
}

f<-c(0.1,0.01,0.001,0.0001)
N<-1e6
b1<-0
b2<-1
b3<--1
s<--log(f)
a<-numeric(4)
for (j in 1:4) {
  a[j]<-fa1(N,b1,b2,b3,f[j])
}

```

```{r}
plot(s,a,xlab = "-logf_0")
```

## 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate
from a normal distribution. Compare the chains generated when different
variances are used for the proposal distribution. Also, compute the
acceptance rates of each chain.

```{r}
set.seed(12445)
rw.Metropolis <- function( sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <=((0.5*exp(-abs(y))) / (0.5*exp(-abs(x[i-1])))))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}


N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 25
rw1 <- rw.Metropolis( sigma[1], x0, N)
rw2 <- rw.Metropolis( sigma[2], x0, N)
rw3 <- rw.Metropolis( sigma[3], x0, N)
rw4 <- rw.Metropolis( sigma[4], x0, N)


```

number of candidate points rejected

```{r}
print(c(rw1$k, rw2$k, rw3$k, rw4$k))

```

acceptance rates of each chain

```{r}
print(c(1-rw1$k/N, 1-rw2$k/N,1-rw3$k/N, 1-rw4$k/N))

```

```{r}
 #display 4 graphs together
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) {
plot(rw[,j], type="l",
xlab=bquote(sigma == .(round(sigma[j],3))),
ylab="X", ylim=range(rw[,j]))

}
#reset to default


```

## 9.7

Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt)
with zero means, unit standard deviations, and correlation 0.9. Plot the
generated sample after discarding a suitable burn-in sample. Fit a
simple linear regression model Y = β0 + β1X to the sample and check the
residuals of the model for normality and constant variance.

```{r}

#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
###### generate the chain #####
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
X[i, 1] <- rnorm(1, m1, s1)
x1 <- X[i, 1]
m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]



```

linear regression model Y = β0 + β1X

```{r}
x1<-x[,1]
y<-x[,2]
model<-lm(y~x1)
summary(model)
```

## 9.10

Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence
of the chain, and run the chain until the chain has converged
approximately to the target distribution according to R \<ˆ 1.2. (See
Exercise 9.9.) Also use the coda [212] package to check for convergence
of the chain by the Gelman-Rubin method. Hints: See the help topics for
the coda functions gelman.diag, gelman.plot, as.mcmc, and mcmc.list.

```{r}

set.seed(1234)
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)

psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.

psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.

v.hat <- W*(n-1)/n + (B/n) #upper variance est.

r.hat <- v.hat / W #G-R statistic

return(r.hat)
}


f <- function(x, sigma) {
if (any(x < 0)) return (0)
stopifnot(sigma > 0)
return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

k <- 4 #number of chains to generate
m<-1e5
X <- matrix(0, nrow=k, ncol=m)

for (l in 1:k) {
  m <- 10000
sigma <- 4
x <- numeric(m)
x[1] <- rchisq(1, df=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1 #y is rejected
}
}

X[l,]<-x
}




k <- 4 #number of chains to generate

b <- 1000 #burn-in length


```

```{r}

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))


#plot the sequence of R-hat statistics
rhat <- rep(0, m)
for (j in (b+1):m)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

```

# HW8

## 1

(1): \n

We notice that the Observed data likelihood is
$$ L(\lambda |u_{i},v_{i})=\prod_{i=1}^{n}(e^{-\lambda u_{i}}-e^{-\lambda v_{i}})$$,So
the optimal point have the equation
$$0=\frac{\partial log(L(\lambda |u_{i},v_{i}))}{\partial \lambda} =-\sum_{i=1}^{n}\frac{u_{i}e^{-\lambda u_{i}}-v_{i}e^{-\lambda v_{i}}}{e^{-\lambda u_{i}}-e^{-\lambda v_{i}}}$$.
\n Then the Complete data likelihood is
$$L(\lambda |u_{i},v_{i},X_{i})=\lambda^{n}\prod_{i=1}^{n}e^{-\lambda X_{i}}1_{X_{i}\in (u_{i},v_{i})} $$.We
also have
$$\pi(X_{i}=x|u_{i},v_{i},\lambda_{k})=\frac{\lambda_{k}e^{-\lambda_{k} x}}{e^{-\lambda_{k} u_{i}}-e^{-\lambda_{k} v_{i}}} 1_{x\in (u_{i},v_{i})}$$.
\n

E-step:$$E[X_{i}|u_{i},v_{i},\lambda_{k}]=\frac{1}{\lambda_{k}} \frac{(1+\lambda_{k}u_{i})e^{-\lambda_{k} u_{i}}-(1+\lambda_{k}v_{i})e^{-\lambda_{k} v_{i}}}{e^{-\lambda_{k} u_{i}}-e^{-\lambda_{k} v_{i}}} $$

\n

M-step:

$$\lambda_{k+1} =\frac{n}{\sum_{i=1}^{n}E[X_{i}|u_{i},v_{i},\lambda_{k}]}\\
=\frac{n\lambda_{k}}{n+\lambda_{k}\sum_{i=1}^{n} \frac{u_{i}e^{-\lambda_{k} u_{i}}-v_{i}e^{-\lambda_{k} v_{i}}}{e^{-\lambda_{k} u_{i}}-e^{-\lambda_{k} v_{i}}}}$$

We replace $\lambda_{k}$,$\lambda_{k+1}$ with $\lambda$ then we can see
that the equation above is equal to the equation that the MLE optimal
satisfies.

\n

Denote
$$h(\lambda)=\sum_{i=1}^{n}\frac{u_{i}e^{-\lambda_{k} u_{i}}-v_{i}e^{-\lambda_{k} v_{i}}}{e^{-\lambda_{k} u_{i}}-e^{-\lambda_{k} v_{i}}}$$,we
have
$$g' (\lambda)=\frac{n^{2}-\lambda nh'(\lambda)-\lambda^{2}nh(\lambda)}{(n+\lambda h(\lambda))^{2}}$$.

Since $\lambda=\lambda_{\infty}$ ensures that \$h(\lambda)=0 \$
and\$h'(\lambda)\in (0,\frac{n}{\lambda}) \$,we get
$g'(\lambda_{\infty})\in(0,1)$ which shows $\lambda_{k}$ converges
linealy. \n

(2) 

```{r,warning=FALSE}
library("stats4")
n<-10

u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)

mlogL <- function(theta=1) {
# minus log-likelihood
return(-(sum(log(exp(-theta*u)-exp(-theta*v)))))
}
fit<-mle(mlogL)

cat("the observed data likelihood estimate",fit@coef,"\n")

g<-function(x){
  c=sum((u*exp(-x*u)-v*exp(-x*v))/(exp(-x*u)-exp(-x*v)))
  return(x*n/(n+x*c))
}
error<-1e-5
a=1
lambda=1

while (a>error) {
  a<-abs(lambda-g(lambda))
  lambda<-g(lambda)
}

cat("the EM estimate",lambda)
```

## 11.8

In the Morra game, the set of optimal strategies are not changed if a
constant is subtracted from every entry of the payoff matrix, or a
positive constant is multiplied times every entry of the payoff matrix.
However, the simplex algorithm may terminate at a different basic
feasible point (also optimal). Compute B \<- A + 2, find the solution of
game B, and verify that it is one of the extreme points (11.12)--(11.15)
of the original game A. Also find the value of game A and game B

```{r}
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
#
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}

A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)

library(boot) #needed for simplex function

B<-A+2

s1<-solve.game(A)
s2<-solve.game(B)






```

the solution of game B

```{r}
 round(cbind(s2$x, s2$y), 7)

```

So it is one of the extreme points (11.12)--(11.15) of the original game
A.

value of game A and game B

```{r}
cat("value of game A:",s1$v,"\t value of game B:",s2$v)


```

# HW9

## 2.1.3 4

Why do you need to use unlist() to convert a list to an atomic vector?
Why doesn't as.vector() work?

Because as.vector will do the function for every column,here is an
example.

```{r}
a<-list(c(1,1,3),c(letters[1:3]))
b<-as.vector(a)
c<-unlist(a)
print(b)
print(c)
```

## 2.3.1 1

What does dim() return when applied to a vector?

```{r}
a<-numeric(10)
print(dim(a))

```

it will return NULL.

## 2.3.1 2

If is.matrix(x) is TRUE, what will is.array(x) return?

```{r}
A<-matrix(1:12,ncol = 3)
print(is.matrix(A))
print(is.array(A))


```

It will return true.

## 2.4.5 2

What does as.matrix() do when applied to a data frame with columns of
different types?

```{r}
a<-1:3
b<-letters[1:3]
C<-data.frame(a,b)
D<-as.matrix(C)
print(D)
```

It will construct the matrix with the type of the highest type of the
columns.

## 2.4.5 3

Can you have a data frame with 0 rows? What about 0 columns?

Both of the question is yes.

```{r}
m<-matrix(ncol = 0,nrow = 0)

d<-data.frame(m)

print(dim(d))

```

## exercise 2

The function below scales a vector so it falls in the range [0, 1]. How
would you apply it to every column of a data frame? How would you apply
it to every numeric column in a data frame?

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
a<-matrix(1:20,nrow = 5)

b<-apply(a, 2, scale01)
print(b)


```

## exercise 1

1.  Use vapply() to:

<!-- -->

a)  Compute the standard deviation of every column in a numeric data
    frame.
b)  Compute the standard deviation of every numeric column in a mixed
    data frame. (Hint: you'll need to use vapply() twice.)

```{r}
a<-rnorm(10)
b<-letters[1:10]
c<-rexp(10)
A<-data.frame(a,c)
B<-data.frame(a,b,c)
vapply(A,FUN=sd,FUN.VALUE = 1)
d<-vapply(B, sd, FUN.VALUE =1)
d[!is.na(d)]
```

## 2

2.  Consider Exercise 9.8 (pages 278, Statistical Computing with R).
    (Hint: Refer to the first example of Case studies section) • Write
    an R function. • Write an Rcpp function. • Compare the computation
    time of the two functions with the function "microbenchmark"

```{r}
library("Rcpp")
library("microbenchmark")

a=0.5
b=0.5
n=20

gibbsR <- function(N, thin) {
  mat <- matrix(nrow = N, ncol = 2)
  x <- y <- 0
  for (i in 1:N) {
    for (j in 1:thin) {
      x <- rbinom(1,n,y)
      y <- rbeta(1,x+a,n+b-x)
    }
    mat[i, ] <- c(x, y)
  }
  mat
}

cppFunction('#include <Rcpp.h>
using namespace Rcpp;
// [[Rcpp::export]]
NumericMatrix gibbsC(int N, int thin) {
  NumericMatrix mat(N, 2);
  double x = 0, y = 0,a=0.5,b=0.5,n=20;
  for(int i = 0; i < N; i++) {
    for(int j = 0; j < thin; j++) {
      x = rbinom(1,n,y)[0];
      y = rbeta(1,x+a,n+b-x)[0];
    }
    mat(i, 0) = x;
    mat(i, 1) = y;
  }
  return(mat);
}
')




ts <- microbenchmark(gibbR=gibbsR(100,10),
gibbC=gibbsC(100,10))
summary(ts)[,c(1,3,5,6)]


```
